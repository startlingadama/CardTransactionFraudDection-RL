{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation - CardTransactionFraudDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import FraudTransactionEnv\n",
    "from models.model import Actor, Critic\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (30,)\n",
      "Reward: -10.0\n"
     ]
    }
   ],
   "source": [
    "env = FraudTransactionEnv(csv_path='data/creditcard.csv')\n",
    "\n",
    "state = env.reset()\n",
    "print(\"State shape:\", state.shape)\n",
    "\n",
    "next_state, reward, done, info = env.step(1)\n",
    "print(f\"Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = state.shape[0]\n",
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.87960000e+04, -8.63977001e-01,  6.12548956e-02,  2.42251402e+00,\n",
       "        8.40051561e-01, -3.58034902e-01,  1.70425697e-01, -5.22626485e-01,\n",
       "        3.86960039e-01,  5.33070833e-01, -3.07751261e-01, -8.87931490e-01,\n",
       "       -8.13670903e-01, -1.79707784e+00, -6.45665182e-02,  1.03029453e+00,\n",
       "       -5.52996507e-01,  5.96961462e-01, -1.61882715e-01,  7.80894899e-01,\n",
       "       -1.14072535e-02,  3.53205862e-02,  1.33946274e-01, -3.16094301e-02,\n",
       "        6.84972645e-02, -4.13154798e-01,  4.61489283e-01, -4.17356412e-02,\n",
       "        1.99105091e-01,  6.43000000e+00])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr = 0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(state):\n",
    "    X = np.zeros(state_dim)\n",
    "    if isinstance(state, tuple):\n",
    "        index = state[0]\n",
    "        X[int(index)] = 1\n",
    "    else:\n",
    "        X[state] = 1\n",
    "    return torch.tensor(X, dtype = torch.float64).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = one_hot(state).to(torch.float)\n",
    "        probs = actor(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = critic(state_tensor)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Calcul des retours\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + 0.99 * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float).unsqueeze(1)\n",
    "    values = torch.cat(values)\n",
    "    advantages = returns - values.detach()\n",
    "\n",
    "    # Mise à jour des modèles\n",
    "    for t in range(len(rewards)):\n",
    "        state_tensor = states[t]\n",
    "        probs = actor(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "        # utiliser la même action qu'au moment de la collecte\n",
    "        new_log_prob = dist.log_prob(actions[t])\n",
    "        ratio = torch.exp(new_log_prob - log_probs[t].detach())\n",
    "        clipped_ratio = torch.clamp(ratio, 0.8, 1.2)\n",
    "\n",
    "        advantage = advantages[t]\n",
    "\n",
    "        surrogate1 = ratio * advantage\n",
    "        surrogate2 = clipped_ratio * advantage\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2)\n",
    "\n",
    "        value = critic(state_tensor)\n",
    "        return_t = returns[t]\n",
    "        value_loss = (value - return_t).pow(2)\n",
    "\n",
    "        entropy = dist.entropy().mean()\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Récompense: {sum(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Teest\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    state_tensor = one_hot(state).to(torch.float)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = actor(state_tensor)\n",
    "    \n",
    "    action = torch.argmax(probs).item()  # greedy choice (exploitation)\n",
    "    \n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Reward obtenue :\", total_reward)\n",
    "\n",
    "state_tensor = one_hot(state).to(torch.float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    value = critic(state_tensor)\n",
    "\n",
    "print(\"Valeur estimée de l’état :\", value.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), \"models/actor.pth\")\n",
    "torch.save(critic.state_dict(), \"models/critic.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load(\"actor_frozenlake.pth\"))\n",
    "critic.load_state_dict(torch.load(\"critic_frozenlake.pth\"))\n",
    "actor.eval()\n",
    "critic.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
