{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation - CardTransactionFraudDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import FraudTransactionEnv\n",
    "from models.model import Actor, Critic\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (30,)\n",
      "Reward: -5.0\n"
     ]
    }
   ],
   "source": [
    "env = FraudTransactionEnv(csv_path='data/creditcard.csv')\n",
    "\n",
    "state = env.reset()\n",
    "print(\"State shape:\", state.shape)\n",
    "\n",
    "next_state, reward, done, info = env.step(1)\n",
    "print(f\"Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = state.shape[0]\n",
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25735000e+05,  2.14309589e+00, -3.86467292e-01, -1.71357875e+00,\n",
       "       -4.28998965e-01,  2.44773347e-01, -5.18825899e-01, -1.62612459e-02,\n",
       "       -1.40988812e-01,  9.53468894e-01, -5.70106552e-03, -1.69817784e+00,\n",
       "       -6.53525832e-01, -1.33757513e+00,  4.38061390e-01,  1.02266152e-01,\n",
       "        9.89071796e-02, -3.06497718e-01, -6.00358401e-01,  7.41126336e-01,\n",
       "       -2.19463718e-01, -3.46699183e-01, -9.56167615e-01,  2.72665760e-01,\n",
       "        4.56717020e-02, -2.87558579e-01,  1.02474867e+00, -1.46339558e-01,\n",
       "       -7.72226850e-02,  1.00000000e+01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "optimizer = optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr = 0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Récompense: 16.0\n",
      "Episode 2, Récompense: 16.0\n",
      "Episode 3, Récompense: 16.0\n",
      "Episode 4, Récompense: 5.0\n",
      "Episode 5, Récompense: 16.0\n",
      "Episode 6, Récompense: 16.0\n",
      "Episode 7, Récompense: 16.0\n",
      "Episode 8, Récompense: 16.0\n",
      "Episode 9, Récompense: 16.0\n",
      "Episode 10, Récompense: 16.0\n",
      "Episode 11, Récompense: 16.0\n",
      "Episode 12, Récompense: 16.0\n",
      "Episode 13, Récompense: 16.0\n",
      "Episode 14, Récompense: 16.0\n",
      "Episode 15, Récompense: 16.0\n",
      "Episode 16, Récompense: 16.0\n",
      "Episode 17, Récompense: 16.0\n",
      "Episode 18, Récompense: 16.0\n",
      "Episode 19, Récompense: 16.0\n",
      "Episode 20, Récompense: 16.0\n",
      "Episode 21, Récompense: 16.0\n",
      "Episode 22, Récompense: 16.0\n",
      "Episode 23, Récompense: 16.0\n",
      "Episode 24, Récompense: 16.0\n",
      "Episode 25, Récompense: 16.0\n",
      "Episode 26, Récompense: 16.0\n",
      "Episode 27, Récompense: 16.0\n",
      "Episode 28, Récompense: 16.0\n",
      "Episode 29, Récompense: 16.0\n",
      "Episode 30, Récompense: 16.0\n",
      "Episode 31, Récompense: 16.0\n",
      "Episode 32, Récompense: 16.0\n",
      "Episode 33, Récompense: 16.0\n",
      "Episode 34, Récompense: 16.0\n",
      "Episode 35, Récompense: 16.0\n",
      "Episode 36, Récompense: 16.0\n",
      "Episode 37, Récompense: 16.0\n",
      "Episode 38, Récompense: 16.0\n",
      "Episode 39, Récompense: 16.0\n",
      "Episode 40, Récompense: 16.0\n",
      "Episode 41, Récompense: 16.0\n",
      "Episode 42, Récompense: 16.0\n",
      "Episode 43, Récompense: 16.0\n",
      "Episode 44, Récompense: 16.0\n",
      "Episode 45, Récompense: 16.0\n",
      "Episode 46, Récompense: 16.0\n",
      "Episode 47, Récompense: 16.0\n",
      "Episode 48, Récompense: 16.0\n",
      "Episode 49, Récompense: 16.0\n",
      "Episode 50, Récompense: 16.0\n",
      "Episode 51, Récompense: 16.0\n",
      "Episode 52, Récompense: 16.0\n",
      "Episode 53, Récompense: 16.0\n",
      "Episode 54, Récompense: 16.0\n",
      "Episode 55, Récompense: 16.0\n",
      "Episode 56, Récompense: 16.0\n",
      "Episode 57, Récompense: 16.0\n",
      "Episode 58, Récompense: 16.0\n",
      "Episode 59, Récompense: 16.0\n",
      "Episode 60, Récompense: 16.0\n",
      "Episode 61, Récompense: 16.0\n",
      "Episode 62, Récompense: 16.0\n",
      "Episode 63, Récompense: 16.0\n",
      "Episode 64, Récompense: 16.0\n",
      "Episode 65, Récompense: 16.0\n",
      "Episode 66, Récompense: 16.0\n",
      "Episode 67, Récompense: 16.0\n",
      "Episode 68, Récompense: 16.0\n",
      "Episode 69, Récompense: 16.0\n",
      "Episode 70, Récompense: 16.0\n",
      "Episode 71, Récompense: 16.0\n",
      "Episode 72, Récompense: 16.0\n",
      "Episode 73, Récompense: 16.0\n",
      "Episode 74, Récompense: 16.0\n",
      "Episode 75, Récompense: 16.0\n",
      "Episode 76, Récompense: 16.0\n",
      "Episode 77, Récompense: 16.0\n",
      "Episode 78, Récompense: 16.0\n",
      "Episode 79, Récompense: 16.0\n",
      "Episode 80, Récompense: 16.0\n",
      "Episode 81, Récompense: 16.0\n",
      "Episode 82, Récompense: 16.0\n",
      "Episode 83, Récompense: 16.0\n",
      "Episode 84, Récompense: 16.0\n",
      "Episode 85, Récompense: 16.0\n",
      "Episode 86, Récompense: 16.0\n",
      "Episode 87, Récompense: 16.0\n",
      "Episode 88, Récompense: 16.0\n",
      "Episode 89, Récompense: 16.0\n",
      "Episode 90, Récompense: 16.0\n",
      "Episode 91, Récompense: 16.0\n",
      "Episode 92, Récompense: 16.0\n",
      "Episode 93, Récompense: 16.0\n",
      "Episode 94, Récompense: 16.0\n",
      "Episode 95, Récompense: 5.0\n",
      "Episode 96, Récompense: 16.0\n",
      "Episode 97, Récompense: 16.0\n",
      "Episode 98, Récompense: 16.0\n",
      "Episode 99, Récompense: 16.0\n",
      "Episode 100, Récompense: 16.0\n",
      "Episode 101, Récompense: 5.0\n",
      "Episode 102, Récompense: 16.0\n",
      "Episode 103, Récompense: 16.0\n",
      "Episode 104, Récompense: 16.0\n",
      "Episode 105, Récompense: 16.0\n",
      "Episode 106, Récompense: 16.0\n",
      "Episode 107, Récompense: 16.0\n",
      "Episode 108, Récompense: 16.0\n",
      "Episode 109, Récompense: 16.0\n",
      "Episode 110, Récompense: 16.0\n",
      "Episode 111, Récompense: 16.0\n",
      "Episode 112, Récompense: 16.0\n",
      "Episode 113, Récompense: 16.0\n",
      "Episode 114, Récompense: 16.0\n",
      "Episode 115, Récompense: 16.0\n",
      "Episode 116, Récompense: 16.0\n",
      "Episode 117, Récompense: 16.0\n",
      "Episode 118, Récompense: 5.0\n",
      "Episode 119, Récompense: 16.0\n",
      "Episode 120, Récompense: 16.0\n",
      "Episode 121, Récompense: 16.0\n",
      "Episode 122, Récompense: 16.0\n",
      "Episode 123, Récompense: 16.0\n",
      "Episode 124, Récompense: 16.0\n",
      "Episode 125, Récompense: 16.0\n",
      "Episode 126, Récompense: 16.0\n",
      "Episode 127, Récompense: 16.0\n",
      "Episode 128, Récompense: 5.0\n",
      "Episode 129, Récompense: 16.0\n",
      "Episode 130, Récompense: 16.0\n",
      "Episode 131, Récompense: 16.0\n",
      "Episode 132, Récompense: 16.0\n",
      "Episode 133, Récompense: 16.0\n",
      "Episode 134, Récompense: 16.0\n",
      "Episode 135, Récompense: 16.0\n",
      "Episode 136, Récompense: 16.0\n",
      "Episode 137, Récompense: 16.0\n",
      "Episode 138, Récompense: 16.0\n",
      "Episode 139, Récompense: 16.0\n",
      "Episode 140, Récompense: 16.0\n",
      "Episode 141, Récompense: 16.0\n",
      "Episode 142, Récompense: 16.0\n",
      "Episode 143, Récompense: 16.0\n",
      "Episode 144, Récompense: 16.0\n",
      "Episode 145, Récompense: 16.0\n",
      "Episode 146, Récompense: 5.0\n",
      "Episode 147, Récompense: 16.0\n",
      "Episode 148, Récompense: 16.0\n",
      "Episode 149, Récompense: 16.0\n",
      "Episode 150, Récompense: 16.0\n",
      "Episode 151, Récompense: 16.0\n",
      "Episode 152, Récompense: 16.0\n",
      "Episode 153, Récompense: 16.0\n",
      "Episode 154, Récompense: 16.0\n",
      "Episode 155, Récompense: 16.0\n",
      "Episode 156, Récompense: 16.0\n",
      "Episode 157, Récompense: 16.0\n",
      "Episode 158, Récompense: 16.0\n",
      "Episode 159, Récompense: 16.0\n",
      "Episode 160, Récompense: 16.0\n",
      "Episode 161, Récompense: 16.0\n",
      "Episode 162, Récompense: 16.0\n",
      "Episode 163, Récompense: 16.0\n",
      "Episode 164, Récompense: 16.0\n",
      "Episode 165, Récompense: 5.0\n",
      "Episode 166, Récompense: 16.0\n",
      "Episode 167, Récompense: 16.0\n",
      "Episode 168, Récompense: 16.0\n",
      "Episode 169, Récompense: 16.0\n",
      "Episode 170, Récompense: 16.0\n",
      "Episode 171, Récompense: 16.0\n",
      "Episode 172, Récompense: 16.0\n",
      "Episode 173, Récompense: 16.0\n",
      "Episode 174, Récompense: 16.0\n",
      "Episode 175, Récompense: 16.0\n",
      "Episode 176, Récompense: 16.0\n",
      "Episode 177, Récompense: 16.0\n",
      "Episode 178, Récompense: 16.0\n",
      "Episode 179, Récompense: 16.0\n",
      "Episode 180, Récompense: 16.0\n",
      "Episode 181, Récompense: 16.0\n",
      "Episode 182, Récompense: 16.0\n",
      "Episode 183, Récompense: 16.0\n",
      "Episode 184, Récompense: 16.0\n",
      "Episode 185, Récompense: 16.0\n",
      "Episode 186, Récompense: 16.0\n",
      "Episode 187, Récompense: 16.0\n",
      "Episode 188, Récompense: 16.0\n",
      "Episode 189, Récompense: 16.0\n",
      "Episode 190, Récompense: 16.0\n",
      "Episode 191, Récompense: 16.0\n",
      "Episode 192, Récompense: 16.0\n",
      "Episode 193, Récompense: 16.0\n",
      "Episode 194, Récompense: 16.0\n",
      "Episode 195, Récompense: 16.0\n",
      "Episode 196, Récompense: 16.0\n",
      "Episode 197, Récompense: 16.0\n",
      "Episode 198, Récompense: 16.0\n",
      "Episode 199, Récompense: 16.0\n",
      "Episode 200, Récompense: 16.0\n",
      "Episode 201, Récompense: 16.0\n",
      "Episode 202, Récompense: 16.0\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    states = []\n",
    "    actions = []\n",
    "\n",
    "    counter = 1\n",
    "    while not done:\n",
    "        state_tensor = state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = actor(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = critic(state_tensor)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "\n",
    "        state = next_state\n",
    "        counter += 1\n",
    "        if counter % 17 == 0:\n",
    "            done = True\n",
    "\n",
    "    # Calcul des retours\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + 0.99 * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float).unsqueeze(1)\n",
    "    values = torch.cat(values)\n",
    "    advantages = returns - values.detach()\n",
    "\n",
    "    # Mise à jour des modèles\n",
    "    for t in range(len(rewards)):\n",
    "        state_tensor = states[t]\n",
    "        probs = actor(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "        # utiliser la même action qu'au moment de la collecte\n",
    "        new_log_prob = dist.log_prob(actions[t])\n",
    "        ratio = torch.exp(new_log_prob - log_probs[t].detach())\n",
    "        clipped_ratio = torch.clamp(ratio, 0.8, 1.2)\n",
    "\n",
    "        advantage = advantages[t]\n",
    "\n",
    "        surrogate1 = ratio * advantage\n",
    "        surrogate2 = clipped_ratio * advantage\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2)\n",
    "\n",
    "        value = critic(state_tensor).squeeze()\n",
    "        return_t = returns[t]\n",
    "        value_loss = (value - return_t).pow(2)\n",
    "\n",
    "        value_loss = value_loss.mean() if value_loss.ndim > 0 else value_loss\n",
    "\n",
    "        entropy = dist.entropy().mean()\n",
    "        policy_loss = policy_loss.mean()\n",
    "        value_loss = value_loss.mean()\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Récompense: {sum(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Teest\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    state_tensor = one_hot(state).to(torch.float)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs = actor(state_tensor)\n",
    "    \n",
    "    action = torch.argmax(probs).item()  # greedy choice (exploitation)\n",
    "    \n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Reward obtenue :\", total_reward)\n",
    "\n",
    "state_tensor = one_hot(state).to(torch.float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    value = critic(state_tensor)\n",
    "\n",
    "print(\"Valeur estimée de l’état :\", value.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), \"models/actor.pth\")\n",
    "torch.save(critic.state_dict(), \"models/critic.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load(\"actor_frozenlake.pth\"))\n",
    "critic.load_state_dict(torch.load(\"critic_frozenlake.pth\"))\n",
    "actor.eval()\n",
    "critic.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
